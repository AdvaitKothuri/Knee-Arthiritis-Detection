{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e894f2e-2f77-4e38-9fa7-381007afd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4225213-b77f-4d41-a8d6-397513ffbc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your data directory\n",
    "data_dir = '/Users/advait/Desktop/Knee_Arthritis_AI/data'\n",
    "class_names = [\"0Normal\", \"1Doubtful\", \"2Mild\", \"3Moderate\", \"4Severe\"]\n",
    "\n",
    "# Initialize lists to hold file paths and labels\n",
    "file_paths = []\n",
    "labels = []\n",
    "\n",
    "# Load file paths and labels with verification for valid images\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        if file_name.endswith(\".png\"):\n",
    "            file_path = os.path.join(class_dir, file_name)\n",
    "            try:\n",
    "                # Check if the file can be opened as an image\n",
    "                with Image.open(file_path) as img:\n",
    "                    img.verify()  # Verify that it is a valid image\n",
    "                file_paths.append(file_path)\n",
    "                labels.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping invalid image: {file_path} - Error: {e}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "file_paths = np.array(file_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Perform stratified split\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    file_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Convert numeric labels to string class names\n",
    "train_labels = [class_names[label] for label in train_labels]\n",
    "val_labels = [class_names[label] for label in val_labels]\n",
    "\n",
    "# Data augmentation for the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    rescale=1.0 / 255.0\n",
    ")\n",
    "val_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "# Create DataFrames for train and validation data\n",
    "train_df = pd.DataFrame({'filename': train_paths, 'class': train_labels})\n",
    "val_df = pd.DataFrame({'filename': val_paths, 'class': val_labels})\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=8,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=8,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load InceptionV3 with pre-trained weights\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze all layers initially\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build the top model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint('best_inceptionv3_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Calculate steps per epoch based on generator batch size\n",
    "steps_per_epoch = len(train_paths) // train_generator.batch_size\n",
    "validation_steps = len(val_paths) // validation_generator.batch_size\n",
    "\n",
    "# Train model for 50 epochs\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[reduce_lr, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7fe19-f3e3-43a6-bfa3-00a38508cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset path\n",
    "dataset_path = '/Users/advait/Desktop/Knee_Arthiritis_AI/data'\n",
    "\n",
    "# Get the list of categories\n",
    "categories = ['0Normal', '1Doubtful', '2Mild', '3Moderate', '4Severe']\n",
    "data = []\n",
    "\n",
    "# Collect data information\n",
    "for category in categories:\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    for filename in os.listdir(category_path):\n",
    "        data.append((category, filename))\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=['Category', 'Filename'])\n",
    "print(df.head())\n",
    "print(df['Category'].value_counts())\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Encoded_Category'] = label_encoder.fit_transform(df['Category'])\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Encoded_Category'])\n",
    "\n",
    "print(train_df.shape, test_df.shape)\n",
    "\n",
    "# Plot the distribution of categories\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='Category')\n",
    "plt.title('Distribution of Knee Arthritis Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d44e36-b77b-4810-ae27-4ff6bfbde39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_predictions(generator, model):\n",
    "    generator.reset()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    steps = len(generator)\n",
    "    for _ in range(steps):\n",
    "        x, y = next(generator)\n",
    "        y_true.extend(np.argmax(y, axis=1))\n",
    "        y_pred.extend(np.argmax(model.predict(x), axis=1))\n",
    "    return np.array(y_true), np.array(y_pred)\n",
    "\n",
    "# Get predictions for training set\n",
    "y_true_train, y_pred_train = get_predictions(train_generator, model)\n",
    "\n",
    "# Get predictions for validation set\n",
    "y_true_val, y_pred_val = get_predictions(validation_generator, model)\n",
    "\n",
    "# Generate classification report for training set\n",
    "classification_report_train = classification_report(y_true_train, y_pred_train, target_names=class_names, zero_division=1)\n",
    "print(\"Classification Report - Training Set:\")\n",
    "print(classification_report_train)\n",
    "\n",
    "# Generate classification report for validation set\n",
    "classification_report_val = classification_report(y_true_val, y_pred_val, target_names=class_names, zero_division=1)\n",
    "print(\"Classification Report - Validation Set:\")\n",
    "print(classification_report_val)\n",
    "\n",
    "# Save the reports to files\n",
    "with open('classification_report_train.txt', 'w') as f:\n",
    "    f.write(\"Classification Report - Training Set:\\n\")\n",
    "    f.write(classification_report_train)\n",
    "\n",
    "with open('classification_report_val.txt', 'w') as f:\n",
    "    f.write(\"Classification Report - Validation Set:\\n\")\n",
    "    f.write(classification_report_val)\n",
    "\n",
    "# Generate and plot confusion matrices\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "plot_confusion_matrix(y_true_train, y_pred_train, \"Confusion Matrix - Training Set\")\n",
    "plot_confusion_matrix(y_true_val, y_pred_val, \"Confusion Matrix - Validation Set\")\n",
    "\n",
    "# Print class distribution\n",
    "print(\"\\nClass Distribution - Training Set:\")\n",
    "print(np.unique(y_true_train, return_counts=True))\n",
    "print(\"\\nClass Distribution - Validation Set:\")\n",
    "print(np.unique(y_true_val, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6134c-97d3-403c-a3e9-c46b2c5e7a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e78f0f-d5a9-4cb6-b39b-826981cd0d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
